{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 01: Supervised Learning\n",
    "\n",
    "### Lesson 08: Model Evaluation Metrics\n",
    "\n",
    "> Learn the main metrics to evaluate models, such as accuracy, precision, recall, and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01. Intro\n",
    "\n",
    "* How well is my model doing?\n",
    "* How do we improve the model based on these metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02. Outline\n",
    "\n",
    "* Problem $\\to$ Tools $\\to$ Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03. Testing your models\n",
    "\n",
    "* Regression and Classification\n",
    "    * Regression returns a numeric value\n",
    "    * Classification returns a state\n",
    "\n",
    "* By testing we can compare the models\n",
    "* Golden rule: Thou shalt never use your testing data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7916666666666666"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import statements \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the train test split\n",
    "# http://scikit-learn.org/0.16/modules/generated/sklearn.cross_validation.train_test_split.html\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read in the data.\n",
    "data = np.asarray(pd.read_csv('../../data/dt_data.csv', header=None))\n",
    "# Assign the features to the variable X, and the labels to the variable y. \n",
    "X = data[:,0:2]\n",
    "y = data[:,2]\n",
    "\n",
    "# Use train test split to split your data \n",
    "# Use a test size of 25% and a random state of 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Instantiate your decision tree model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# TODO: Fit the model to the training data.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# TODO: Calculate the accuracy and assign it to the variable acc on the test data.\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 04. Confusion Matrix\n",
    "\n",
    "* Confusion Matrix(混淆矩阵)\n",
    "    * The true positives are the points that are positive and the model correctly labels as positive.\n",
    "    * The true negatives are the points that are negative and the model correctly labels as negative.\n",
    "    * The false positives are the points that are negative, but the model incorrectly labels as positive.\n",
    "    * The false negatives are the points that are positive but the modeling says negative.\n",
    "\n",
    "|Confusion Matrix |Guessed Positive|Guessed Negative|\n",
    "| :--: | :--: | :--: |\n",
    "|Positive|True Positives|False Negatives|\n",
    "|Negative|False Positives|True Negatives|\n",
    "\n",
    "* Type 1 Error (Error of the first kind, or False Positive): In the medical example, this is when we misdiagnose a healthy patient as sick.\n",
    "* Type 2 Error (Error of the second kind, or False Negative): In the medical example, this is when we misdiagnose a sick patient as healthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 05. Confusion Matrix 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 06. Accuracy\n",
    "\n",
    "* Accuracy(精度) is a ratio between correctly classified points and the number of total points\n",
    "* $Accuracy = \\frac{\\text{Correctly classified points}}{\\text{All points}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 07. Accuracy 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 08. When accuracy won't work\n",
    "\n",
    "* Credit card fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 09. False Negatives and Positives\n",
    "\n",
    "* In the medical example, False Negative is much worse than a False Positive.\n",
    "* In the spam detector example, a False Positive is much worse than a False Negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Precision\n",
    "\n",
    "$Precision(查准率) = \\frac{TP}{TP+FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Recall\n",
    "\n",
    "$Recall(查全率)=\\frac{TP}{TP+FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. F1 Score\n",
    "\n",
    "* we use `Harmonic mean`(调和平均) instead of `arithmetic mean`(算术平均)\n",
    "* F1 score(F1度量) is closer to the smallest between precision and recall\n",
    "$F_1 score = 2 * \\frac{Precision*Recall}{Precision+Recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. F-beta Score\n",
    "\n",
    "* $F_{\\beta} score = (1+\\beta^2) * \\frac{Precision*Recall}{\\beta^2 * Precision+Recall}$ (加权调和平均)\n",
    "* When $\\beta$ = 0, $F_{\\beta}=Precision$\n",
    "* When $\\beta$ goes to infinity, $F_{\\beta} score = \\frac{Precision*Recall}{\\frac{\\beta^2}{1+\\beta^2}*Precision+\\frac{1}{1+\\beta^2}*Recall}=Recall$\n",
    "* $\\beta$ < 1, Precision more impact; $\\beta$ > 1, Recall more impact; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. ROC Curve\n",
    "\n",
    "* ROC(Receiver Operating Characteristic, 受试者工作特征曲线)\n",
    "\n",
    "What we want is to come up with a metric or some number that is high for the perfect split, medium for the good split, and low for the random split.\n",
    "\n",
    "$TPR(\\text{True Positive Rate}) = \\frac{TP}{TP+FN} \\\\\n",
    "FPR(\\text{False Positive Rate}) = \\frac{FP}{TN+FP}$\n",
    "\n",
    "* AUC(Area Under ROC Curve)\n",
    "    * Random Split: Area(AUC) = 0.5\n",
    "    * Good Split: 0.5 < Area(AUC) < 1\n",
    "    * Perfect Split: Area(AUC) = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
