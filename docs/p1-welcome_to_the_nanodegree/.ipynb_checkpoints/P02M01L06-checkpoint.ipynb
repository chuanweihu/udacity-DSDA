{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 01: Supervised Learning\n",
    "\n",
    "### Lesson 06: Support Vector Machines\n",
    "\n",
    "> Support vector machines are a common method used for classification problems. They have been proven effective using what is known as the 'kernel' trick!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01. Intro\n",
    "\n",
    "Support vector machines are a very powerful algorithm for classification that not only aims to classify the data, but it also aims to find the best possible boundary, namely, the one that maintains the largest distance from the points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02. Which line is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03. Minimizing Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 04. Error Function Intuition\n",
    "\n",
    "$Error = Classification Error + Margin Error$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 05. Perceptron Algorithm\n",
    "\n",
    "Perceptron algorithm will be to minimize this error using gradient descent in order to find the ideal W and b that give us the best possible cut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 06. Classification Error\n",
    "\n",
    "The error starts from the bottom line and top line seperately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 07. Margin Error\n",
    "\n",
    "* Large margin, small error; Small margin, large error\n",
    "* $Margin = \\frac{2}{|W|}$\n",
    "* $Error = |W|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 08. (Optional) Margin Error Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 09. Error Function\n",
    "\n",
    "We minimize error function using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. The C Parameter\n",
    "\n",
    "$Error = C * Classification Error + Margin Error$ (Constant)\n",
    "* Small C $\\to$ large margin $\\to$ May make classification errors\n",
    "* Large C $\\to$ small margin $\\to$ May make classifies points well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Polynomial Kernel 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Polynomial Kernel 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Polynomial Kernel 3\n",
    "\n",
    "A kernel means a set of functions that will come to help us out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. RBF Kernel 1\n",
    "\n",
    "RBF: radial basis functions kernel(Gaussian kernel function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. RBF Kernel 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16. RBF Kernel 3\n",
    "\n",
    "* Large $\\gamma$ tend to overfit, small $\\gamma$ tend to underfit\n",
    "* Normal Distribution: $y = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^-{\\frac{(x-\\mu)^2}{2\\sigma^2}}$\n",
    "* $\\gamma = \\frac{1}{2\\sigma^2}$\n",
    "\n",
    "As long as we think of gamma as some parameter that is associated with the width of the curve in an inverse way, then we are grasping the concept of the gamma parameter and the RBF kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17. SVMs in sklearn\n",
    "\n",
    "**Support Vector Machines in sklearn**\n",
    "\n",
    "For your support vector machine model, you'll be using scikit-learn's SVC class. This class provides the functions to define and fit the model to your data.\n",
    "\n",
    "```\n",
    ">>> from sklearn.svm import SVC\n",
    ">>> model = SVC()\n",
    ">>> model.fit(x_values, y_values)\n",
    "```\n",
    "\n",
    "In the example above, the model variable is a support vector machine model that has been fitted to the data x_values and y_values. Fitting the model means finding the best boundary that fits the training data. Let's make two predictions using the model's predict() function.\n",
    "\n",
    "```\n",
    ">>> print(model.predict([ [0.2, 0.8], [0.5, 0.4] ]))\n",
    "[[ 0., 1.]]\n",
    "```\n",
    "\n",
    "The model returned an array of predictions, one prediction for each input array. The first input, [0.2, 0.8], got a prediction of 0.. The second input, [0.5, 0.4], got a prediction of 1..\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "When we define the model, we can specify the hyperparameters. As we've seen in this section, the most common ones are\n",
    "\n",
    "* `C`: The C parameter.\n",
    "* `kernel`: The kernel. The most common ones are 'linear', 'poly', and 'rbf'.\n",
    "* `degree`: If the kernel is polynomial, this is the maximum degree of the monomials in the kernel.\n",
    "* `gamma`: If the kernel is rbf, this is the gamma parameter.\n",
    "\n",
    "For example, here we define a model with a polynomial kernel of degree 4, and a C parameter of 0.1.\n",
    "```\n",
    ">>> model = SVC(kernel='poly', degree=4, C=0.1)\n",
    "```\n",
    "\n",
    "**Support Vector Machines Quiz**\n",
    "\n",
    "You'll need to complete each of the following steps:\n",
    "\n",
    "1. Build a support vector machine model\n",
    "    * Create a support vector machine classification model using scikit-learn's SVC and assign it to the variablemodel.\n",
    "2. Fit the model to the data\n",
    "    * If necessary, specify some of the hyperparameters. The goal is to obtain an accuracy of 100% in the dataset. Hint: Not every kernel will work well.\n",
    "3. Predict using the model\n",
    "    * Predict the labels for the training set, and assign this list to the variable y_pred.\n",
    "4. Calculate the accuracy of the model\n",
    "    * For this, use the function sklearn function accuracy_score.\n",
    "\n",
    "When you hit Test Run, you'll be able to see the boundary region of your model, which will help you tune the correct parameters, in case you need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import statements \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the data.\n",
    "data = np.asarray(pd.read_csv('../../data/svm_data.csv', header=None))\n",
    "# Assign the features to the variable X, and the labels to the variable y. \n",
    "X = data[:,0:2]\n",
    "y = data[:,2]\n",
    "\n",
    "# TODO: Create the model and assign it to the variable model.\n",
    "# Find the right parameters for this model to achieve 100% accuracy on the dataset.\n",
    "model = SVC(kernel='rbf', gamma=27)\n",
    "\n",
    "# TODO: Fit the model.\n",
    "model.fit(X, y)\n",
    "\n",
    "# TODO: Make predictions. Store them in the variable y_pred.\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# TODO: Calculate the accuracy and assign it to the variable acc.\n",
    "acc = accuracy_score(y, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18. Recap & Additional Resources\n",
    "\n",
    "**Recap**\n",
    "\n",
    "In this lesson, you learned about Support Vector Machines (or SVMs). SVMs are a popular algorithm used for classification problems. You saw three different ways that SVMs can be implemented:\n",
    "\n",
    "* Maximum Margin Classifier\n",
    "* Classification with Inseparable Classes\n",
    "* Kernel Methods\n",
    "\n",
    "1. Maximum Margin Classifier\n",
    "\n",
    "When your data can be completely separated, the `linear version of SVMs` attempts to maximize the distance from the linear boundary to the closest points (called the `support vectors`).\n",
    "\n",
    "2. Classification with Inseparable Classes\n",
    "\n",
    "Unfortunately, data in the real world is rarely completely separable. For this reason, we introduced a new `hyper-parameter called C`. The C hyper-parameter determines `how flexible we are willing to be with the points that fall on the wrong side of our dividing boundary`. The value of C ranges between 0 and infinity. When C is large, you are forcing your boundary to have fewer errors than when it is a small value. (`Small C, Large margin; Large C, Small margin`)\n",
    "\n",
    "Note: when `C is too large` for a particular set of data, you `might not get convergence` at all because your data cannot be separated with the small number of errors allotted with such a large value of C.\n",
    "\n",
    "3. Kernels\n",
    "\n",
    "Finally, we looked at what makes SVMs truly powerful, kernels. Kernels in SVMs allow us the ability to separate data when the boundary between them is `nonlinear`. Specifically, you saw two types of kernels:\n",
    "\n",
    "* polynomial\n",
    "* rbf\n",
    "\n",
    "By far the most popular kernel is the rbf kernel (which stands for radial basis function). The rbf kernel allows you the opportunity to classify points that seem hard to separate in any space. This is a density based approach that looks at the closeness of points to one another. This introduces another `hyper-parameter gamma`. When gamma is large, the outcome is similar to having a large value of C, that is your algorithm will attempt to classify every point correctly. Alternatively, small values of gamma will try to cluster in a more general way that will make more mistakes, but may perform better when it sees new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
